{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2b5d3f4",
   "metadata": {},
   "source": [
    "# Install required libraries for solution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed914c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pickle5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b976f0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cloudpickle==2.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a01d535",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym==0.21.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81313393",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym-retro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69177ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-contrib-python --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac51d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f396c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e371682",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install stable-baselines3[extra] optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab83405c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install protobuf==3.20.1 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fc6115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105c3c4a",
   "metadata": {},
   "source": [
    "## Import all required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c956e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle5 as pickle\n",
    "# Import retro to play Street Fighter using a ROM\n",
    "import retro\n",
    "# Import time to slow down game\n",
    "import time\n",
    "\n",
    "# Import environment base class for a wrapper \n",
    "from gym import Env \n",
    "# Import the space shapes for the environment\n",
    "from gym.spaces import MultiBinary, Box\n",
    "# Import numpy to calculate frame delta \n",
    "import numpy as np\n",
    "# Import opencv for grayscaling\n",
    "import cv2\n",
    "# Import matplotlib for plotting the image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Importing the optimzation frame - HPO\n",
    "import optuna\n",
    "# Import os to deal with filepaths\n",
    "import os\n",
    "# PPO algo for RL\n",
    "from stable_baselines3 import PPO\n",
    "# Bring in the eval policy method for metric calculation\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "# Import the sb3 monitor for logging \n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "# Import the vec wrappers to vectorize and frame stack\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
    "from stable_baselines3 import DQN\n",
    "from typing import Any, Dict\n",
    "\n",
    "# Import base callback \n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "from gym import spaces\n",
    "import gym\n",
    "\n",
    "# used for creating date in required formate\n",
    "from datetime import date\n",
    "\n",
    "#To be able to log custom scalars\n",
    "import tensorflow as tf\n",
    "from tensorboard.plugins.hparams import api as hp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32981f76",
   "metadata": {},
   "source": [
    "## Setup paths to log , record agent game play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d58e2109",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = './logs_final/final_results_statical_dnq/'\n",
    "OPT_DIR = './opt/final_results_statical_ppo/'\n",
    "CHECKPOINT_DIR = './train/final_results_statical_dnq/'\n",
    "model_name = 'final_results_statical_dnq_' + date.today().strftime('%Y-%m-%d')\n",
    "RECORD_PATH= './RecordAgentsGamePlay/final_results_statical_dnq'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acf7d27",
   "metadata": {},
   "source": [
    "# Create game environment wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05b8029d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adapted class called StreetFighter from https://github.com/nicknochnack/StreetFighterRL/blob/main/StreetFighter-Tutorial.ipynb\n",
    "#Made changes to the __init__ function to allow the user the option to record the game play , changed it work for Alien Soldier\n",
    "#enviroment , added logic to set buttons and game state name.\n",
    "#Made changes to reset function added own custom attributes which needed to be reset once game restarts  \n",
    "#Made changes to step function added logic which sets penalties and to update custom attributes created.\n",
    "\n",
    "# Create custom environment \n",
    "class AlienSoldier(Env): \n",
    "    def __init__(self,record_results=False,record_path=''):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Specify action space and observation space \n",
    "        self.observation_space = Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
    "        self.action_space =  spaces.MultiBinary(12)\n",
    "        # Set the buttons as they needed when wrapped with AlienSoldierDiscretizer\n",
    "        self.buttons = ['B', 'A', 'MODE', 'START', 'UP', 'DOWN', 'LEFT', 'RIGHT', 'C', 'Y', 'X', 'Z']\n",
    "    \n",
    "        # Startup and instance of the game \n",
    "        if record_results:\n",
    "            self.game = retro.make(game='AlienSoldier-Genesis', use_restricted_actions=retro.Actions.FILTERED, record=record_path)\n",
    "        else:\n",
    "            self.game = retro.make(game='AlienSoldier-Genesis', use_restricted_actions=retro.Actions.FILTERED)\n",
    "            \n",
    "        #set the game state name so able to access which level the game is on\n",
    "        self.statename = self.game.statename\n",
    "    \n",
    "    def reset(self):\n",
    "        # Return the first frame \n",
    "        obs = self.game.reset()\n",
    "        obs = self.preprocess(obs) \n",
    "        self.previous_frame = obs \n",
    "        self.statename = self.game.statename\n",
    "        # reset attribute which holds the score delta , health , time for next game \n",
    "        self.score = 0 \n",
    "        self.health = 512\n",
    "        self.time = 157\n",
    "        \n",
    "        return obs\n",
    "    \n",
    "    def preprocess(self, observation): \n",
    "        # Convert observation to gray scale \n",
    "        gray = cv2.cvtColor(observation, cv2.COLOR_BGR2GRAY)\n",
    "        # Resize the frame\n",
    "        resize = cv2.resize(gray, (84,84), interpolation=cv2.INTER_CUBIC)\n",
    "        # Add the channels value\n",
    "        channels = np.reshape(resize, (84,84,1))\n",
    "        return channels \n",
    "    \n",
    "    def step(self, action): \n",
    "        \n",
    "        # Take a step \n",
    "        obs, reward, done, info = self.game.step(action)\n",
    "        obs = self.preprocess(obs) \n",
    "        \n",
    "        # Frame delta \n",
    "        frame_delta = obs - self.previous_frame\n",
    "        self.previous_frame = obs \n",
    "        \n",
    "        # Reshape the reward function\n",
    "        reward = info['score'] - self.score \n",
    "        \n",
    "        # Add in penalties \n",
    "        if self.health < info['health'] :\n",
    "            reward = - 10\n",
    "        \n",
    "        if self.health > info['health'] :\n",
    "            reward = 20\n",
    "        \n",
    "        if self.time != info['time']:\n",
    "            reward = -5\n",
    "            \n",
    "        # update custom attributes \n",
    "        self.health = info['health']\n",
    "        self.time =  info['time'] \n",
    "        self.score = info['score'] \n",
    "        \n",
    "        return frame_delta, reward, done, info\n",
    "    \n",
    "    def render(self, *args, **kwargs):\n",
    "        self.game.render()\n",
    "        \n",
    "    def close(self):\n",
    "        self.game.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07df62f",
   "metadata": {},
   "source": [
    "## Build wrapper for Gym Env so actions space is combo's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c5f71cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code not mine taken from https://github.com/openai/retro/blob/master/retro/examples/discretizer.py \n",
    "#Made be called AlienSoldierDiscretizer instead of SonicDiscretizer , also changed the combo list added in the class to \n",
    "#be one that could be used in Alien Soldier\n",
    "\n",
    "class Discretizer(gym.ActionWrapper):\n",
    "    \"\"\"\n",
    "    Wrap a gym environment and make it use discrete actions.\n",
    "    Args:\n",
    "        combos: ordered list of lists of valid button combinations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, combos):\n",
    "        super().__init__(env)\n",
    "        assert isinstance(env.action_space, gym.spaces.MultiBinary)\n",
    "        buttons = env.unwrapped.buttons\n",
    "        self._decode_discrete_action = []\n",
    "        for combo in combos:\n",
    "            arr = np.array([False] * env.action_space.n)\n",
    "            for button in combo:\n",
    "                arr[buttons.index(button)] = True\n",
    "            self._decode_discrete_action.append(arr)\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(len(self._decode_discrete_action))\n",
    "\n",
    "    def action(self, act):\n",
    "        return self._decode_discrete_action[act].copy()\n",
    "\n",
    "\n",
    "class AlienSoldierDiscretizer(Discretizer):\n",
    "    \"\"\"\n",
    "    based on https://github.com/openai/retro-baselines/blob/master/agents/sonic_util.py\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        # Added own combo list\n",
    "        super().__init__(env=env, combos=[['LEFT'], \n",
    "                                          ['RIGHT'],\n",
    "                                          ['C'],\n",
    "                                          ['C','LEFT'],\n",
    "                                          ['C','RIGHT'],\n",
    "                                          ['B'],\n",
    "                                          ['B','B'],\n",
    "                                          ['LEFT','B','B'],\n",
    "                                          ['RIGHT','B','B']\n",
    "                                         ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689e9ab6",
   "metadata": {},
   "source": [
    "## Helper functions for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f79d106c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/utils/hyperparams_opt.py\n",
    "# Made a adjustment to buffer_size so within range lapetop could handel\n",
    "# Return test hyperparameters for DNQ model\n",
    "def optimize_dqn(trial: optuna.Trial) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Sampler for DQN hyperparams.\n",
    "    :param trial:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    gamma = trial.suggest_categorical(\"gamma\", [0.9, 0.95, 0.98, 0.99, 0.995, 0.999, 0.9999])\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64, 100, 128, 256, 512])\n",
    "    buffer_size = trial.suggest_categorical(\"buffer_size\", [int(1e4), int(2e4), int(3e4),int(5e4)])\n",
    "    exploration_final_eps = trial.suggest_uniform(\"exploration_final_eps\", 0, 0.2)\n",
    "    exploration_fraction = trial.suggest_uniform(\"exploration_fraction\", 0, 0.5)\n",
    "    target_update_interval = trial.suggest_categorical(\"target_update_interval\", [1, 1000, 5000, 10000, 15000, 20000])\n",
    "    learning_starts = trial.suggest_categorical(\"learning_starts\", [0, 1000, 5000, 10000, 20000])\n",
    "\n",
    "    train_freq = trial.suggest_categorical(\"train_freq\", [1, 4, 8, 16, 128, 256, 1000])\n",
    "    subsample_steps = trial.suggest_categorical(\"subsample_steps\", [1, 2, 4, 8])\n",
    "    gradient_steps = max(train_freq // subsample_steps, 1)\n",
    "\n",
    "    net_arch = trial.suggest_categorical(\"net_arch\", [\"tiny\", \"small\", \"medium\"])\n",
    "\n",
    "    net_arch = {\"tiny\": [64], \"small\": [64, 64], \"medium\": [256, 256]}[net_arch]\n",
    "\n",
    "    hyperparams = {\n",
    "        \"gamma\": gamma,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"buffer_size\": buffer_size,\n",
    "        \"train_freq\": train_freq,\n",
    "        \"gradient_steps\": gradient_steps,\n",
    "        \"exploration_fraction\": exploration_fraction,\n",
    "        \"exploration_final_eps\": exploration_final_eps,\n",
    "        \"target_update_interval\": target_update_interval,\n",
    "        \"learning_starts\": learning_starts,\n",
    "        \"policy_kwargs\": dict(net_arch=net_arch),\n",
    "    }\n",
    "\n",
    "    return hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89ec4876",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taken from https://github.com/nicknochnack/StreetFighterRL/blob/main/StreetFighter-Tutorial.ipynb\n",
    "# Return test hyperparameters for PPO model\n",
    "def optimize_ppo(trial): \n",
    "    return {\n",
    "        'n_steps':trial.suggest_int('n_steps', 2048, 8192),\n",
    "        'gamma':trial.suggest_loguniform('gamma', 0.8, 0.9999),\n",
    "        'learning_rate':trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
    "        'clip_range':trial.suggest_uniform('clip_range', 0.1, 0.4),\n",
    "        'gae_lambda':trial.suggest_uniform('gae_lambda', 0.8, 0.99)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6dfba03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adapted from https://github.com/nicknochnack/StreetFighterRL/blob/main/StreetFighter-Tutorial.ipynb\n",
    "# Runs a trail and returns the mean reward \n",
    "def optimize_agent(trial,model_algorithm):\n",
    "    try: \n",
    "\n",
    "        # Create environment \n",
    "        env = AlienSoldier()\n",
    "        \n",
    "        # Depending on algorithm will need to call a different function to get trial params\n",
    "        if model_algorithm == \"dnq\":\n",
    "            \n",
    "            model_params = optimize_dqn(trial)\n",
    "            #The DNQ needs the action space to be Discrete but AlienSoldier has it as MultiBinary so we need wrap it \n",
    "            # with AlienSoldierDiscretizer which makes the action space Discrete\n",
    "            env = AlienSoldierDiscretizer(env)\n",
    "        else:\n",
    "            model_params = optimize_ppo(trial)\n",
    "        \n",
    "        # Wrap the env with wrappers \n",
    "        env = Monitor(env, LOG_DIR)\n",
    "        env = DummyVecEnv([lambda: env])\n",
    "        env = VecFrameStack(env, 4, channels_order='last')\n",
    "\n",
    "        # Create model with required algorithm\n",
    "        if model_algorithm == \"dnq\":\n",
    "            model = DQN(\"CnnPolicy\", env, tensorboard_log=LOG_DIR, verbose=0, **model_params)\n",
    "        else:\n",
    "            model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=0, **model_params)\n",
    "            \n",
    "        #Train model for required num of steps\n",
    "        model.learn(total_timesteps=100000)\n",
    "#         model.learn(total_timesteps=100)\n",
    "        \n",
    "        # Evaluate model \n",
    "        mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=5)\n",
    "        env.close()\n",
    "\n",
    "        # Save the model for later use if user decides to use model as base \n",
    "        SAVE_PATH = os.path.join(OPT_DIR, 'trial_{}_best_model'.format(trial.number))\n",
    "        \n",
    "        model.save(SAVE_PATH)\n",
    "\n",
    "        return mean_reward\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"--------------------------\")\n",
    "        print(\"Error\" , e)\n",
    "        print(\"--------------------------\")\n",
    "        return -1000\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa75e62",
   "metadata": {},
   "source": [
    "## Set algorithm for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdafe026",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_algorithm=\"dnq\"\n",
    "# Taken from https://www.kaggle.com/general/261870\n",
    "# Wrap the optimize_agent inside a lambda reason need to pass more args to optimize_agent\n",
    "func_optimize_agent = lambda trial: optimize_agent(trial,model_algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec30b63c",
   "metadata": {},
   "source": [
    "## Find the best hyperparameter for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a51bb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-09-04 18:37:58,663]\u001b[0m A new study created in memory with name: no-name-d27e3536-20a3-416d-b257-13e334d3da4e\u001b[0m\n",
      "C:\\Users\\Tiger\\.conda\\envs\\game\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:141: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 2541`, after every 39 untruncated mini-batches, there will be a truncated mini-batch of size 45\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=2541 and n_envs=1)\n",
      "  f\"You have specified a mini-batch size of {batch_size},\"\n",
      "\u001b[32m[I 2022-09-04 18:45:47,700]\u001b[0m Trial 0 finished with value: 45.0 and parameters: {'n_steps': 2541, 'gamma': 0.9336208019550793, 'learning_rate': 2.4697426775067584e-05, 'clip_range': 0.1946541666231457, 'gae_lambda': 0.8627205620027374}. Best is trial 0 with value: 45.0.\u001b[0m\n",
      "C:\\Users\\Tiger\\.conda\\envs\\game\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:141: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 3686`, after every 57 untruncated mini-batches, there will be a truncated mini-batch of size 38\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=3686 and n_envs=1)\n",
      "  f\"You have specified a mini-batch size of {batch_size},\"\n",
      "\u001b[32m[I 2022-09-04 18:53:22,979]\u001b[0m Trial 1 finished with value: 75.0 and parameters: {'n_steps': 3686, 'gamma': 0.955177724605964, 'learning_rate': 4.943397443677692e-05, 'clip_range': 0.2599423012090678, 'gae_lambda': 0.8508566377984575}. Best is trial 1 with value: 75.0.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Creating the experiment \n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(func_optimize_agent, n_trials=100, n_jobs=1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dc38d2",
   "metadata": {},
   "source": [
    "## Get best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dfeef31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_steps': 3686,\n",
       " 'gamma': 0.955177724605964,\n",
       " 'learning_rate': 4.943397443677692e-05,\n",
       " 'clip_range': 0.2599423012090678,\n",
       " 'gae_lambda': 0.8508566377984575}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729d587d",
   "metadata": {},
   "source": [
    "## Get best trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "951cff6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenTrial(number=1, values=[75.0], datetime_start=datetime.datetime(2022, 9, 4, 18, 45, 47, 700424), datetime_complete=datetime.datetime(2022, 9, 4, 18, 53, 22, 979137), params={'n_steps': 3686, 'gamma': 0.955177724605964, 'learning_rate': 4.943397443677692e-05, 'clip_range': 0.2599423012090678, 'gae_lambda': 0.8508566377984575}, distributions={'n_steps': IntUniformDistribution(high=8192, low=2048, step=1), 'gamma': LogUniformDistribution(high=0.9999, low=0.8), 'learning_rate': LogUniformDistribution(high=0.0001, low=1e-05), 'clip_range': UniformDistribution(high=0.4, low=0.1), 'gae_lambda': UniformDistribution(high=0.99, low=0.8)}, user_attrs={}, system_attrs={}, intermediate_values={}, trial_id=1, state=TrialState.COMPLETE, value=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_trial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6375387",
   "metadata": {},
   "source": [
    "## Best hyperparameters I used to trained for algorithm model\n",
    "\n",
    "### DNQ \n",
    "\n",
    "```\n",
    "model_params =  {'gamma': 0.99, \n",
    "                 'learning_rate': 0.0024514414607983074, \n",
    "                 'batch_size': 512, \n",
    "                 'buffer_size': 50000, \n",
    "                 'exploration_final_eps': 0.13882656073419855, \n",
    "                 'exploration_fraction': 0.0636702480159673, \n",
    "                 'target_update_interval': 20000, \n",
    "                 'learning_starts': 1000, \n",
    "                 'train_freq': 16\n",
    "                 }\n",
    "```\n",
    "\n",
    "## PPO \n",
    "```\n",
    "model_params = {'n_steps': 4153,\n",
    "                'gamma': 0.8881436120036623,\n",
    "                'learning_rate': 2.8578237422936424e-05,\n",
    "                 'clip_range': 0.18402687390457662,\n",
    "                 'gae_lambda': 0.8780780193937191\n",
    "                }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7726b594",
   "metadata": {},
   "source": [
    "## Setup callback function for logging to tensorboard_log for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fd9930c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from https://github.com/nicknochnack/StreetFighterRL/blob/main/StreetFighter-Tutorial.ipynb\n",
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43cc62fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the callback function to be used in the model tensorboard_log\n",
    "callback = TrainAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e6a452",
   "metadata": {},
   "source": [
    "# Train model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7e9ffa",
   "metadata": {},
   "source": [
    "### Create enviroment to train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5e593a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment \n",
    "env = AlienSoldier()\n",
    "\n",
    "# Depending on algorithm may need to wrap it with AlienSoldierDiscretizer\n",
    "if model_algorithm == \"dnq\":\n",
    "    #The DNQ needs the action space to be Discrete but AlienSoldier has it as MultiBinary so we need wrap it \n",
    "    # with AlienSoldierDiscretizer which makes the action space Discrete\n",
    "    env = AlienSoldierDiscretizer(env)\n",
    "\n",
    "env = Monitor(env, LOG_DIR)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, 4, channels_order='last')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9020120c",
   "metadata": {},
   "source": [
    "### Setup model params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e775000f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If your not going to load a model from hyperparameter tuning prcoess then you will need weights to use for model\n",
    "#Below are params I used to train my model , users maybe be different according to best params \n",
    "#they get for there study on Optuna\n",
    "if model_algorithm == \"dnq\":\n",
    "    model_params =  {\n",
    "                     'gamma': 0.99, \n",
    "                     'learning_rate': 0.0024514414607983074, \n",
    "                     'batch_size': 512, 'buffer_size': 50000, \n",
    "                     'exploration_final_eps': 0.13882656073419855, \n",
    "                     'exploration_fraction': 0.0636702480159673, \n",
    "                     'target_update_interval': 20000, \n",
    "                     'learning_starts': 1000, \n",
    "                     'train_freq': 16\n",
    "                    }\n",
    "else:\n",
    "    #please note n_steps needs be divisible by 64 so that field may need to be updated on params taken from best trail \n",
    "    model_params = {'n_steps': 4153,\n",
    "                     'gamma': 0.8881436120036623,\n",
    "                     'learning_rate': 2.8578237422936424e-05,\n",
    "                     'clip_range': 0.18402687390457662,\n",
    "                     'gae_lambda': 0.8780780193937191\n",
    "                   }\n",
    "    #Making n_steps divisible by 64\n",
    "    model_params['n_steps'] = 4160"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47634c6c",
   "metadata": {},
   "source": [
    "### Create model for training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b19418b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tiger\\.conda\\envs\\game\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:221: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 2.82GB > 0.68GB\n",
      "  \"This system does not have apparently enough memory to store the complete \"\n"
     ]
    }
   ],
   "source": [
    "if model_algorithm == \"dnq\":\n",
    "    model =  DQN(\"CnnPolicy\", env, tensorboard_log=LOG_DIR, verbose=1, **model_params)\n",
    "else:\n",
    "    model =  PPO(\"CnnPolicy\", env, tensorboard_log=LOG_DIR, verbose=1, **model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c650803",
   "metadata": {},
   "source": [
    "### Load prevoius model to take advantage of previous weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47dc2ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1f797aced08>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load previous model\n",
    "#Pass in the best model here \n",
    "model.load(os.path.join(OPT_DIR, 'trial_1_best_model.zip'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da3a03b",
   "metadata": {},
   "source": [
    "### Start training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b40a5018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./logs/statical_test_ppo/PPO_4\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 3.22e+03 |\n",
      "|    ep_rew_mean     | 206      |\n",
      "| time/              |          |\n",
      "|    fps             | 63       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 58       |\n",
      "|    total_timesteps | 3712     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1f7ad210c08>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start training the model\n",
    "model.learn(total_timesteps=5000000, callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebce72c",
   "metadata": {},
   "source": [
    "## Evaluate the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "da2129bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update to the best model from training\n",
    "if model_algorithm == \"dnq\":\n",
    "    model = DQN.load(os.path.join(CHECKPOINT_DIR, 'best_model_10000000.zip'))\n",
    "else:\n",
    "    model = PPO.load(os.path.join(CHECKPOINT_DIR, 'best_model_7060000.zip'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "67f3d894",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, _ = evaluate_policy(model, env, render=True, n_eval_episodes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70e4ab2",
   "metadata": {},
   "source": [
    "### Print the mean reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5ad139ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1381.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc36e7a",
   "metadata": {},
   "source": [
    "## Start agent playing game for required amount of games "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e4b4d7",
   "metadata": {},
   "source": [
    "## Helper functions for agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "936fb41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to retrun the level from state name in the retro enviroment. The state name contains alot string data \n",
    "# around it e.g DefaultSettings.Level1.state only interested in the level int as would like to log which level agnet ends on\n",
    "def getLevelFromStateName(state_name):\n",
    "    \n",
    "    # initializing substrings\n",
    "    start_str = \"evel\"\n",
    "    end_str = \".state\"\n",
    " \n",
    "    # getting index of substrings\n",
    "    start_index = state_name.find(start_str)\n",
    "    end_index = state_name.find(end_str)\n",
    "    \n",
    "    # sub string the level from state name as we only want level as number \n",
    "    level = state_name[start_index + len(start_str): end_index]\n",
    "\n",
    "    # convert the level to int and retrun it \n",
    "    return int(level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d36f7e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to start game environment and have the statical agent play the number games passed to function  \n",
    "def staticalAgentPlayGame(LOG_DIR,model_name,RECORD_PATH,RecordGamePlay,NumerOfGamesToPlay,\n",
    "                          model_algorithm,model_to_Load):\n",
    "        \n",
    "    # Starts up the game environment\n",
    "    \n",
    "    # Create environment \n",
    "    env = AlienSoldier(record_results=RecordGamePlay,record_path=RECORD_PATH)\n",
    "\n",
    "    # Depending on algorithm may need to wrap it with AlienSoldierDiscretizer\n",
    "    if model_algorithm == \"dnq\":\n",
    "        #The DNQ needs the action space to be Discrete but AlienSoldier has it as MultiBinary so we need wrap it \n",
    "        # with AlienSoldierDiscretizer which makes the action space Discrete\n",
    "        env = AlienSoldierDiscretizer(env)\n",
    "\n",
    "    env = Monitor(env, LOG_DIR)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    env = VecFrameStack(env, 4, channels_order='last')\n",
    "    \n",
    "    # Create model\n",
    "    if model_algorithm == \"dnq\":\n",
    "        model =  DQN(\"CnnPolicy\", env, tensorboard_log=LOG_DIR, verbose=1, **model_params)\n",
    "#         model = DQN.load(os.path.join(CHECKPOINT_DIR, 'best_model_5000000.zip'))\n",
    "        model = DQN.load(os.path.join(CHECKPOINT_DIR, model_to_Load))\n",
    "    else:\n",
    "        model =  PPO(\"CnnPolicy\", env, tensorboard_log=LOG_DIR, verbose=1, **model_params)\n",
    "#         model = PPO.load(os.path.join(CHECKPOINT_DIR, 'best_model_1000000.zip'))\n",
    "        model = PPO.load(os.path.join(CHECKPOINT_DIR, model_to_Load))\n",
    "\n",
    "    # Setup writer which will log the scalar values from end of agent game play \n",
    "    writer = tf.summary.create_file_writer(LOG_DIR,name=model_name)\n",
    "\n",
    "    print(\"------------------------------------\")\n",
    "    # Reset game to starting state\n",
    "    obs = env.reset()\n",
    "    # Set done flag to flase this indicates if agent game is done\n",
    "    done = False\n",
    "\n",
    "    # Loop over number games to play   \n",
    "    for game in range(NumerOfGamesToPlay): \n",
    "        print(\"Game Num\" ,game + 1)\n",
    "\n",
    "        #Check if agents game has ended \n",
    "        while not done: \n",
    "            # Render the game frame \n",
    "            env.render()\n",
    "            # Predict the next action the agent should perform\n",
    "            action = model.predict(obs)[0]\n",
    "            \n",
    "            \n",
    "            #Set the values from step function\n",
    "            obs, reward, done, info = env.step(action)    \n",
    "\n",
    "            #Check if game is done, if true then proceed to log scalar values to writer to be logged \n",
    "            if done: \n",
    "                print(\"Game over\")\n",
    "                print(\"states\",info[0])\n",
    "                \n",
    "    \n",
    "                # Log the values to writer \n",
    "                with writer.as_default():\n",
    "                    tf.summary.scalar(\"score\", info[0][\"score\"], step=game + 1)\n",
    "                    tf.summary.scalar(\"time\", info[0][\"time\"], step=game + 1)\n",
    "                    tf.summary.scalar(\"health\", info[0][\"health\"], step=game + 1)\n",
    "                    tf.summary.scalar(\"level\", getLevelFromStateName(env.get_attr(\"statename\")[0]), step=game + 1)\n",
    "                    writer.flush()\n",
    "                print(\"------------------------------------\")\n",
    "        # Reset obs so game can restart\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531a70b9",
   "metadata": {},
   "source": [
    "## Start game enviroment and start agent playing game for required amount of games "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d4e5eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tiger\\.conda\\envs\\game\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:221: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 2.82GB > 0.55GB\n",
      "  \"This system does not have apparently enough memory to store the complete \"\n",
      "C:\\Users\\Tiger\\.conda\\envs\\game\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:221: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 0.56GB > 0.49GB\n",
      "  \"This system does not have apparently enough memory to store the complete \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "Game Num 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tiger\\.conda\\envs\\game\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:406: UserWarning: [WinError -2147417850] Cannot change thread mode after it is set\n",
      "  warnings.warn(str(err))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game over\n",
      "states {'health': 0, 'score': 3063, 'time': 115, 'episode': {'r': 583, 'l': 2598, 't': 227.963948}, 'terminal_observation': array([[[0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0]]], dtype=uint8)}\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# If you get a error of Cannot create multiple emulator instances per process,please run last code block in solution\n",
    "staticalAgentPlayGame(LOG_DIR,model_name,RECORD_PATH,True,1,model_algorithm,'best_model_5000000.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca94d737",
   "metadata": {},
   "source": [
    "## Convert game play to mp4\n",
    "\n",
    "Update the below cmd to path where your game play is saved "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfa896c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -m retro.scripts.playback_movie RecordAgentsGamePlay/final_results_statical_dnq/AlienSoldier-Genesis-DefaultSettings.Level1-000001.bk2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e949b1",
   "metadata": {},
   "source": [
    "## Run below block if get error of Cannot create multiple emulator instances per process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9b6e9fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52c9447",
   "metadata": {},
   "source": [
    "##  End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Game Retro",
   "language": "python",
   "name": "game"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
